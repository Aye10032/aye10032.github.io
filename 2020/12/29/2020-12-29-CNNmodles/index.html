<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/images/favicon.png"><link rel="icon" href="/images/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Aye10032"><meta name="keywords" content=""><meta name="description" content="自用笔记，几种经典CNN模型的分析"><meta property="og:type" content="article"><meta property="og:title" content="几种经典的CNN模型"><meta property="og:url" content="https://www.aye10032.com/2020/12/29/2020-12-29-CNNmodles/index.html"><meta property="og:site_name" content="鸽子窝"><meta property="og:description" content="自用笔记，几种经典CNN模型的分析"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.aye10032.com/images/posts/CNNmodels/LeNet.png"><meta property="og:image" content="https://www.aye10032.com/images/posts/CNNmodels/AlexNet.png"><meta property="og:image" content="https://www.aye10032.com/images/posts/CNNmodels/VGGNet.png"><meta property="og:image" content="https://www.aye10032.com/images/posts/CNNmodels/InceptionNet.png"><meta property="og:image" content="https://www.aye10032.com/images/posts/CNNmodels/ResNet.png"><meta property="article:published_time" content="2020-12-29T00:00:00.000Z"><meta property="article:modified_time" content="2023-03-15T10:57:21.000Z"><meta property="article:author" content="Aye10032"><meta property="article:tag" content="学习"><meta property="article:tag" content="机器学习"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://www.aye10032.com/images/posts/CNNmodels/LeNet.png"><meta name="twitter:creator" content="@Aye10032"><meta name="referrer" content="no-referrer-when-downgrade"><title>几种经典的CNN模型 - 鸽子窝</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"www.aye10032.com",root:"/",version:"1.9.5",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/images/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:null,google:{measurement_id:"G-285089DP1E"},tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>Fluid.ctx.dnt||Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=G-285089DP1E",(function(){function a(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],a("js",new Date),a("config","G-285089DP1E")}))</script><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>鸽子窝</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item"><a class="nav-link" href="/bangumis/index.html"><i class="iconfont icon-bilibili-fill"></i> <span>我的追番</span></a></li><li class="nav-item"><a class="nav-link" href="/links/"><i class="iconfont icon-link-fill"></i> <span>友链</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/images/background-cover.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="几种经典的CNN模型"></span></div><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> Aye10032 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2020-12-29 00:00" pubdate>2020年12月29日 凌晨</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 17k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 146 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="学习" id="heading-4ef520d6cd20ba4a727af08e17e4939e" role="tab" data-toggle="collapse" href="#collapse-4ef520d6cd20ba4a727af08e17e4939e" aria-expanded="true">学习 <span class="list-group-count">(12)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-4ef520d6cd20ba4a727af08e17e4939e" role="tabpanel" aria-labelledby="heading-4ef520d6cd20ba4a727af08e17e4939e"><div class="category-post-list"></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="unity" id="heading-439b3a25b555b3bc8667a09a036ae70c" role="tab" data-toggle="collapse" href="#collapse-439b3a25b555b3bc8667a09a036ae70c" aria-expanded="false">unity <span class="list-group-count">(1)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-439b3a25b555b3bc8667a09a036ae70c" role="tabpanel" aria-labelledby="heading-439b3a25b555b3bc8667a09a036ae70c"><div class="category-post-list"><a href="/2023/01/21/2023-01-21-Unity-Particle/" title="unity 粒子效果笔记" class="list-group-item list-group-item-action"><span class="category-post">unity 粒子效果笔记</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="期末复习" id="heading-69f506b4426d281814d067770ad53047" role="tab" data-toggle="collapse" href="#collapse-69f506b4426d281814d067770ad53047" aria-expanded="false">期末复习 <span class="list-group-count">(7)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-69f506b4426d281814d067770ad53047" role="tabpanel" aria-labelledby="heading-69f506b4426d281814d067770ad53047"><div class="category-post-list"><a href="/2018/05/20/2018-05-20-PLC/" title="PLC期末题库" class="list-group-item list-group-item-action"><span class="category-post">PLC期末题库</span> </a><a href="/2018/05/18/2018-05-18-dawu/" title="大学物理上复习笔记" class="list-group-item list-group-item-action"><span class="category-post">大学物理上复习笔记</span> </a><a href="/2020/06/28/2020-06-28-IoT/" title="工业物联网复习" class="list-group-item list-group-item-action"><span class="category-post">工业物联网复习</span> </a><a href="/2018/05/05/2018-05-05-feC/" title="工程材料学复习笔记" class="list-group-item list-group-item-action"><span class="category-post">工程材料学复习笔记</span> </a><a href="/2020/06/13/2020-06-13-Gcode/" title="机床数控复习" class="list-group-item list-group-item-action"><span class="category-post">机床数控复习</span> </a><a href="/categories/%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/" class="list-group-item list-group-item-action"><span class="category-post">More...</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem list-group-item category-item-action col-10 col-md-11 col-xm-11" title="机器学习" id="heading-61bb1751fd355596e307767d1927c855" role="tab" data-toggle="collapse" href="#collapse-61bb1751fd355596e307767d1927c855" aria-expanded="true">机器学习 <span class="list-group-count">(1)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-61bb1751fd355596e307767d1927c855" role="tabpanel" aria-labelledby="heading-61bb1751fd355596e307767d1927c855"><div class="category-post-list"><a href="/2020/12/29/2020-12-29-CNNmodles/" title="几种经典的CNN模型" class="list-group-item list-group-item-action active"><span class="category-post">几种经典的CNN模型</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="生物信息学" id="heading-9b17337cce03384cfde4fda8da0ff881" role="tab" data-toggle="collapse" href="#collapse-9b17337cce03384cfde4fda8da0ff881" aria-expanded="false">生物信息学 <span class="list-group-count">(3)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-9b17337cce03384cfde4fda8da0ff881" role="tabpanel" aria-labelledby="heading-9b17337cce03384cfde4fda8da0ff881"><div class="category-post-list"><a href="/2023/08/28/2023-08-28-sam_format/" title="sam/bam文件格式信息" class="list-group-item list-group-item-action"><span class="category-post">sam/bam文件格式信息</span> </a><a href="/2023/08/11/2023-08-11-trim_goal/" title="测序数据处理自用笔记" class="list-group-item list-group-item-action"><span class="category-post">测序数据处理自用笔记</span> </a><a href="/2023/05/24/2023-05-24-Bioinformatics/" title="生物信息学知识图谱" class="list-group-item list-group-item-action"><span class="category-post">生物信息学知识图谱</span></a></div></div></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">几种经典的CNN模型</h1><p class="note note-info">本文最后更新于：2023年3月15日 上午</p><div class="markdown-body"><h1 id="几种经典的cnn模型">几种经典的CNN模型</h1><p>首先，标准的CNN网络结构模型一般包括：</p><ul><li>卷积层</li><li>批标准化</li><li>激活函数</li><li>池化</li><li>dropout，休眠一定比例的神经元</li></ul><h2 id="lenet">1、LeNet</h2><p>共5层，包括两层卷积计算和三层全连接</p><ul><li>第一层卷积计算<ul><li>6个5*5卷积核，卷积步长为1，不使用全零填充</li><li>无批标准化</li><li>激活函数使用sigmoid</li><li>最大池化，池化核尺寸2*2，步长为2，不使用全零填充</li><li>无dropout</li></ul></li><li>第二层卷积计算<ul><li>16个5*5卷积核，卷积步长为1，不使用全零填充</li><li>无批标准化</li><li>激活函数使用sigmoid</li><li>最大池化，池化核尺寸2*2，步长为2，不使用全零填充</li><li>无dropout</li></ul></li><li>fatten</li><li>全连接网络，120个神经元，使用sigmoid激活函数</li><li>全连接网络，84个神经元，使用sigmoid激活函数</li><li>全连接网络，10个神经元，使用softmax激活函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BaseModel</span>(<span class="hljs-title class_ inherited__">Model</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(BaseModel, self).__init__()<br>        <span class="hljs-comment"># 卷积（特征提取）</span><br>        <span class="hljs-comment"># C:卷积层--6个卷积核，每个尺寸为5*5</span><br>        self.c1 = Conv2D(filters=<span class="hljs-number">6</span>, kernel_size=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), padding=<span class="hljs-string">&#x27;valid&#x27;</span>)  <br>        <span class="hljs-comment"># A:激活层--激活</span><br>        self.a1 = Activation(tf.keras.activations.sigmoid)  <br>        <span class="hljs-comment"># P: 池化层--最大池化，尺寸2*2，步长2</span><br>        self.p1 = MaxPool2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">&#x27;valid&#x27;</span>)  <br><br>        self.c2 = Conv2D(filters=<span class="hljs-number">6</span>, kernel_size=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), padding=<span class="hljs-string">&#x27;valid&#x27;</span>)<br>        self.a2 = Activation(tf.keras.activations.sigmoid)<br>        self.p2 = MaxPool2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">&#x27;valid&#x27;</span>)<br><br>        self.flatten = Flatten()<br>        self.f1 = Dense(<span class="hljs-number">120</span>, activation=tf.keras.activations.sigmoid)<br>        self.f2 = Dense(<span class="hljs-number">84</span>, activation=tf.keras.activations.sigmoid)<br>        self.f3 = Dense(<span class="hljs-number">10</span>, activation=tf.keras.activations.softmax)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs, training=<span class="hljs-literal">None</span>, mask=<span class="hljs-literal">None</span></span>):<br>        inputs = self.c1(inputs)<br>        inputs = self.a1(inputs)<br>        inputs = self.p1(inputs)<br><br>        inputs = self.c2(inputs)<br>        inputs = self.a2(inputs)<br>        inputs = self.p2(inputs)<br><br>        inputs = self.flatten(inputs)<br>        inputs = self.f1(inputs)<br>        inputs = self.f2(inputs)<br>        outputs = self.f3(inputs)<br><br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure><figure><img src="/images/posts/CNNmodels/LeNet.png" srcset="/images/loading.gif" lazyload alt="LeNet"><figcaption aria-hidden="true">LeNet</figcaption></figure><h2 id="alexnet">2、AlexNet</h2><p>共8层，包括五层卷积计算和三层全连接</p><ul><li>第一层卷积计算<ul><li>96个3*3卷积核，卷积步长为1，不使用全零填充</li><li>使用局部相应标准化LRN</li><li>激活函数使用relu</li><li>最大池化，池化核尺寸3*3，步长为2</li><li>无dropout</li></ul></li><li>第二层卷积计算<ul><li>256个3*3卷积核，卷积步长为1，不使用全零填充</li><li>使用局部相应标准化LRN</li><li>激活函数使用relu</li><li>最大池化，池化核尺寸3*3，步长为2</li><li>无dropout</li></ul></li><li>第三层卷积计算<ul><li>384个3*3卷积核，卷积步长为1，<strong>使用</strong>全零填充</li><li>无标准化</li><li>激活函数使用relu</li><li>不使用池化</li><li>无dropout</li></ul></li><li>第四层卷积计算与第三层相同<ul><li>384个3*3卷积核，卷积步长为1，<strong>使用</strong>全零填充</li><li>无标准化</li><li>激活函数使用relu</li><li>不使用池化</li><li>无dropout</li></ul></li><li>第五层卷积计算<ul><li>256个3*3卷积核，卷积步长为1，<strong>使用</strong>全零填充</li><li>无标准化</li><li>激活函数使用relu</li><li>最大池化，池化核尺寸3*3，步长为2</li><li>无dropout</li></ul></li><li>fatten</li><li>全连接网络，2048个神经元，使用relu激活函数，50% dropout</li><li>全连接网络，2048个神经元，使用relu激活函数，50% dropout</li><li>全连接网络，10个神经元，使用softmax激活函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BaseModel</span>(<span class="hljs-title class_ inherited__">Model</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(BaseModel, self).__init__()<br>        self.c1 = Conv2D(filters=<span class="hljs-number">96</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;valid&#x27;</span>)<br>        self.b1 = BatchNormalization()<br>        self.a1 = Activation(tf.keras.activations.relu)<br>        self.p1 = MaxPool2D(pool_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), strides=<span class="hljs-number">2</span>)<br><br>        self.c2 = Conv2D(filters=<span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;valid&#x27;</span>)<br>        self.b2 = BatchNormalization()<br>        self.a2 = Activation(tf.keras.activations.relu)<br>        self.p2 = MaxPool2D(pool_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), strides=<span class="hljs-number">2</span>)<br><br>        self.c3 = Conv2D(filters=<span class="hljs-number">384</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>        self.a3 = Activation(tf.keras.activations.relu)<br><br>        self.c4 = Conv2D(filters=<span class="hljs-number">384</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>        self.a4 = Activation(tf.keras.activations.relu)<br><br>        self.c5 = Conv2D(filters=<span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>        self.a5 = Activation(tf.keras.activations.relu)<br>        self.p5 = MaxPool2D(pool_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), strides=<span class="hljs-number">2</span>)<br><br>        self.flatten = Flatten()<br>        self.f1 = Dense(<span class="hljs-number">2048</span>, activation=tf.keras.activations.relu)<br>        self.d1 = Dropout(<span class="hljs-number">0.5</span>)<br>        self.f2 = Dense(<span class="hljs-number">2048</span>, activation=tf.keras.activations.relu)<br>        self.d2 = Dropout(<span class="hljs-number">0.5</span>)<br>        self.f3 = Dense(<span class="hljs-number">10</span>, activation=tf.keras.activations.softmax)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs, training=<span class="hljs-literal">None</span>, mask=<span class="hljs-literal">None</span></span>):<br>        inputs = self.c1(inputs)<br>        inputs = self.b1(inputs)<br>        inputs = self.a1(inputs)<br>        inputs = self.p1(inputs)<br><br>        inputs = self.c2(inputs)<br>        inputs = self.b2(inputs)<br>        inputs = self.a2(inputs)<br>        inputs = self.p2(inputs)<br><br>        inputs = self.c3(inputs)<br>        inputs = self.a3(inputs)<br><br>        inputs = self.c4(inputs)<br>        inputs = self.a4(inputs)<br><br>        inputs = self.c5(inputs)<br>        inputs = self.a5(inputs)<br>        inputs = self.p5(inputs)<br><br>        inputs = self.flatten(inputs)<br>        inputs = self.f1(inputs)<br>        inputs = self.d1(inputs)<br>        inputs = self.f2(inputs)<br>        inputs = self.d2(inputs)<br>        outputs = self.f3(inputs)<br><br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure><figure><img src="/images/posts/CNNmodels/AlexNet.png" srcset="/images/loading.gif" lazyload alt="image-20201231132729733"><figcaption aria-hidden="true">image-20201231132729733</figcaption></figure><h2 id="vggnet">3、VGGNet</h2><p>共16层，包括13层卷积计算及三层全连接</p><h3 id="卷积计算层">1、卷积计算层</h3><table><thead><tr class="header"><th style="text-align:center">层数</th><th style="text-align:center">卷积层</th><th style="text-align:center">批标准化</th><th style="text-align:center">激活层</th><th style="text-align:center">池化层</th><th style="text-align:center">dropout</th></tr></thead><tbody><tr class="odd"><td style="text-align:center">1</td><td style="text-align:center">64个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="even"><td style="text-align:center">2</td><td style="text-align:center">64个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">最大池化，池化核尺寸2*2，池化步长为2</td><td style="text-align:center">0.2</td></tr><tr class="odd"><td style="text-align:center">3</td><td style="text-align:center">128个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="even"><td style="text-align:center">4</td><td style="text-align:center">128个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">最大池化，池化核尺寸2*2，池化步长为2</td><td style="text-align:center">0.2</td></tr><tr class="odd"><td style="text-align:center">5</td><td style="text-align:center">256个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="even"><td style="text-align:center">6</td><td style="text-align:center">256个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="odd"><td style="text-align:center">7</td><td style="text-align:center">256个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">最大池化，池化核尺寸2*2，池化步长为2</td><td style="text-align:center">0.2</td></tr><tr class="even"><td style="text-align:center">8</td><td style="text-align:center">512个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="odd"><td style="text-align:center">9</td><td style="text-align:center">512个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="even"><td style="text-align:center">10</td><td style="text-align:center">512个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">最大池化，池化核尺寸2*2，池化步长为2</td><td style="text-align:center">0.2</td></tr><tr class="odd"><td style="text-align:center">11</td><td style="text-align:center">512个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="even"><td style="text-align:center">12</td><td style="text-align:center">512个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="odd"><td style="text-align:center">13</td><td style="text-align:center">512个卷积核，尺寸为3*3，步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">最大池化，池化核尺寸2*2，池化步长为2</td><td style="text-align:center">0.2</td></tr></tbody></table><h3 id="全连接层">2、全连接层</h3><ul><li>fatten</li><li>全连接网络，512个神经元，使用relu激活函数，20% dropout</li><li>全连接网络，512个神经元，使用relu激活函数，20% dropout</li><li>全连接网络，10个神经元，使用softmax激活函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(BaseModel, self).__init__()<br>    self.c1 = Conv2D(filters=<span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b1 = BatchNormalization()<br>    self.a1 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br><br>    self.c2 = Conv2D(filters=<span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b2 = BatchNormalization()<br>    self.a2 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br>    self.p2 = MaxPool2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.d2 = Dropout(<span class="hljs-number">0.2</span>)<br><br>    self.c3 = Conv2D(filters=<span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b3 = BatchNormalization()<br>    self.a3 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br><br>    self.c4 = Conv2D(filters=<span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b4 = BatchNormalization()<br>    self.a4 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br>    self.p4 = MaxPool2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.d4 = Dropout(<span class="hljs-number">0.2</span>)<br><br>    self.c5 = Conv2D(filters=<span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b5 = BatchNormalization()<br>    self.a5 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br><br>    self.c6 = Conv2D(filters=<span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b6 = BatchNormalization()<br>    self.a6 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br><br>    self.c7 = Conv2D(filters=<span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b7 = BatchNormalization()<br>    self.a7 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br>    self.p7 = MaxPool2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.d7 = Dropout(<span class="hljs-number">0.2</span>)<br><br>    self.c8 = Conv2D(filters=<span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b8 = BatchNormalization()<br>    self.a8 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br><br>    self.c9 = Conv2D(filters=<span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b9 = BatchNormalization()<br>    self.a9 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br><br>    self.c10 = Conv2D(filters=<span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b10 = BatchNormalization()<br>    self.a10 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br>    self.p10 = MaxPool2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.d10 = Dropout(<span class="hljs-number">0.2</span>)<br><br>    self.c11 = Conv2D(filters=<span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b11 = BatchNormalization()<br>    self.a11 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br><br>    self.c12 = Conv2D(filters=<span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b12 = BatchNormalization()<br>    self.a12 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br><br>    self.c13 = Conv2D(filters=<span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.b13 = BatchNormalization()<br>    self.a13 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br>    self.p13 = MaxPool2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>    self.d13 = Dropout(<span class="hljs-number">0.2</span>)<br><br>    self.flatten = Flatten()<br>    self.fa = Dense(<span class="hljs-number">512</span>, activation=tf.keras.activations.relu)<br>    self.da = Dropout(<span class="hljs-number">0.2</span>)<br>    self.fb = Dense(<span class="hljs-number">512</span>, activation=tf.keras.activations.relu)<br>    self.db = Dropout(<span class="hljs-number">0.2</span>)<br>    self.fc = Dense(<span class="hljs-number">10</span>, activation=tf.keras.activations.softmax)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs, training=<span class="hljs-literal">None</span>, mask=<span class="hljs-literal">None</span></span>):<br>    inputs = self.c1(inputs)<br>    inputs = self.b1(inputs)<br>    inputs = self.a1(inputs)<br><br>    inputs = self.c2(inputs)<br>    inputs = self.b2(inputs)<br>    inputs = self.a2(inputs)<br>    inputs = self.p2(inputs)<br>    inputs = self.d2(inputs)<br><br>    inputs = self.c3(inputs)<br>    inputs = self.b3(inputs)<br>    inputs = self.a3(inputs)<br><br>    inputs = self.c4(inputs)<br>    inputs = self.b4(inputs)<br>    inputs = self.a4(inputs)<br>    inputs = self.p4(inputs)<br>    inputs = self.d4(inputs)<br><br>    inputs = self.c5(inputs)<br>    inputs = self.b5(inputs)<br>    inputs = self.a5(inputs)<br><br>    inputs = self.c6(inputs)<br>    inputs = self.b6(inputs)<br>    inputs = self.a6(inputs)<br><br>    inputs = self.c7(inputs)<br>    inputs = self.b7(inputs)<br>    inputs = self.a7(inputs)<br>    inputs = self.p7(inputs)<br>    inputs = self.d7(inputs)<br><br>    inputs = self.c8(inputs)<br>    inputs = self.b8(inputs)<br>    inputs = self.a8(inputs)<br><br>    inputs = self.c9(inputs)<br>    inputs = self.b9(inputs)<br>    inputs = self.a9(inputs)<br><br>    inputs = self.c10(inputs)<br>    inputs = self.b10(inputs)<br>    inputs = self.a10(inputs)<br>    inputs = self.p10(inputs)<br>    inputs = self.d10(inputs)<br><br>    inputs = self.c11(inputs)<br>    inputs = self.b11(inputs)<br>    inputs = self.a11(inputs)<br><br>    inputs = self.c12(inputs)<br>    inputs = self.b12(inputs)<br>    inputs = self.a12(inputs)<br><br>    inputs = self.c13(inputs)<br>    inputs = self.b13(inputs)<br>    inputs = self.a13(inputs)<br>    inputs = self.p13(inputs)<br>    inputs = self.d13(inputs)<br><br>    inputs = self.flatten(inputs)<br>    inputs = self.fa(inputs)<br>    inputs = self.da(inputs)<br>    inputs = self.fb(inputs)<br>    inputs = self.db(inputs)<br>    outputs = self.fc(inputs)<br><br>    <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure><figure><img src="/images/posts/CNNmodels/VGGNet.png" srcset="/images/loading.gif" lazyload alt="VGGNet"><figcaption aria-hidden="true">VGGNet</figcaption></figure><h2 id="inceptionnet">4、InceptionNet</h2><p>主体结构为Inception结构块，结构如下图所示：</p><pre><code class="mermaid">classDiagram
卷积连接器 &lt;-- 1X1卷积核a
卷积连接器 &lt;-- 3X3卷积核b
3X3卷积核b &lt;-- 1X1卷积核b : 降维
卷积连接器 &lt;-- 5X5卷积核c
5X5卷积核c &lt;-- 1X1卷积核c : 降维
卷积连接器 &lt;-- 1X1卷积核d : 降维
1X1卷积核d &lt;-- 3X3最大池化核
1X1卷积核a &lt;-- 输入层
1X1卷积核b &lt;-- 输入层
1X1卷积核c &lt;-- 输入层
3X3最大池化核 &lt;-- 输入层
卷积连接器 : 将收到的四路特征
卷积连接器 : 按照深度拼接
</code></pre><table><thead><tr class="header"><th style="text-align:center">卷积核</th><th style="text-align:center">卷积层</th><th style="text-align:center">批标准化</th><th style="text-align:center">激活层</th><th style="text-align:center">池化层</th><th style="text-align:center">dropout</th></tr></thead><tbody><tr class="odd"><td style="text-align:center">1X1卷积核a</td><td style="text-align:center">16个卷积核，尺寸为1*1，<br>步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="even"><td style="text-align:center">1X1卷积核b</td><td style="text-align:center">16个卷积核，尺寸为1*1，<br>步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="odd"><td style="text-align:center">3X3卷积核b</td><td style="text-align:center">16个卷积核，尺寸为3*3，<br>步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="even"><td style="text-align:center">1X1卷积核c</td><td style="text-align:center">16个卷积核，尺寸为1*1，<br>步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="odd"><td style="text-align:center">5X5卷积核c</td><td style="text-align:center">16个卷积核，尺寸为5*5，<br>步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr class="even"><td style="text-align:center">3X3最大池化核</td><td style="text-align:center">无</td><td style="text-align:center">无</td><td style="text-align:center">无</td><td style="text-align:center">最大池化，池化核尺寸3*3，<br>池化步长为1，全零填充</td><td style="text-align:center">无</td></tr><tr class="odd"><td style="text-align:center">1X1卷积核d</td><td style="text-align:center">16个卷积核，尺寸为1*1，<br>步长为1，全零填充</td><td style="text-align:center">BN操作</td><td style="text-align:center">relu</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr></tbody></table><p>神经网络结构范例如下所示，包括四个Inception结构块，两两一组，分为block_0和block_1</p><pre><code class="mermaid">graph TB
id1(3*3 conv, filters=16)
style id1 fill: #3991B620, stroke: #3991B6, stroke-width:2px
id2_1(Filter concatenation)
id2_2(1*1 conv)
id2_3(1*1 conv)
id2_4(3*3 conv)
id2_5(1*1 conv)
id2_6(5*5 conv)
id2_7(1*1 conv)
id2_8(3*3 pooling)
id2_9(Previous layer)
style id2_1 fill: #B9CF9320, stroke: #B9CF93, stroke-width:2px
style id2_9 fill: #B9CF9320, stroke: #B9CF93, stroke-width:2px
style id2_2 fill: #3991B620, stroke: #3991B6, stroke-width:2px
style id2_4 fill: #3991B620, stroke: #3991B6, stroke-width:2px
style id2_6 fill: #3991B620, stroke: #3991B6, stroke-width:2px
style id2_3 fill: #FFFF6940, stroke: #FFFF69, stroke-width:2px
style id2_5 fill: #FFFF6940, stroke: #FFFF69, stroke-width:2px
style id2_7 fill: #FFFF6940, stroke: #FFFF69, stroke-width:2px
style id2_8 fill: #CFA3A420, stroke: #CFA3A4, stroke-width:2px
id3_1(Filter concatenation)
id3_2(1*1 conv)
id3_3(1*1 conv)
id3_4(3*3 conv)
id3_5(1*1 conv)
id3_6(5*5 conv)
id3_7(1*1 conv)
id3_8(3*3 pooling)
id3_9(Previous layer)
style id3_1 fill: #B9CF9320, stroke: #B9CF93, stroke-width:2px
style id3_9 fill: #B9CF9320, stroke: #B9CF93, stroke-width:2px
style id3_2 fill: #3991B620, stroke: #3991B6, stroke-width:2px
style id3_4 fill: #3991B620, stroke: #3991B6, stroke-width:2px
style id3_6 fill: #3991B620, stroke: #3991B6, stroke-width:2px
style id3_3 fill: #FFFF6940, stroke: #FFFF69, stroke-width:2px
style id3_5 fill: #FFFF6940, stroke: #FFFF69, stroke-width:2px
style id3_7 fill: #FFFF6940, stroke: #FFFF69, stroke-width:2px
style id3_8 fill: #CFA3A420, stroke: #CFA3A4, stroke-width:2px
subgraph block_0
id2_9 --&gt; id2_2
id2_2 --&gt; id2_1
id2_9 --&gt; id2_3
id2_3 --&gt; id2_4
id2_4 --&gt; id2_1
id2_9 --&gt; id2_5
id2_5 --&gt; id2_6
id2_6 --&gt; id2_1
id2_9 --&gt; id2_8
id2_8 --&gt; id2_7
id2_7 --&gt; id2_1
id3_9 --&gt; id3_2
id3_2 --&gt; id3_1
id3_9 --&gt; id3_3
id3_3 --&gt; id3_4
id3_4 --&gt; id3_1
id3_9 --&gt; id3_5
id3_5 --&gt; id3_6
id3_6 --&gt; id3_1
id3_9 --&gt; id3_8
id3_8 --&gt; id3_7
id3_7 --&gt; id3_1
id2_1 --&gt; id3_9
end
id4_1(Filter concatenation)
id4_2(1*1 conv)
id4_3(1*1 conv)
id4_4(3*3 conv)
id4_5(1*1 conv)
id4_6(5*5 conv)
id4_7(1*1 conv)
id4_8(3*3 pooling)
id4_9(Previous layer)
style id4_1 fill: #B9CF9320, stroke: #B9CF93, stroke-width:2px
style id4_9 fill: #B9CF9320, stroke: #B9CF93, stroke-width:2px
style id4_2 fill: #3991B620, stroke: #3991B6, stroke-width:2px
style id4_4 fill: #3991B620, stroke: #3991B6, stroke-width:2px
style id4_6 fill: #3991B620, stroke: #3991B6, stroke-width:2px
style id4_3 fill: #FFFF6940, stroke: #FFFF69, stroke-width:2px
style id4_5 fill: #FFFF6940, stroke: #FFFF69, stroke-width:2px
style id4_7 fill: #FFFF6940, stroke: #FFFF69, stroke-width:2px
style id4_8 fill: #CFA3A420, stroke: #CFA3A4, stroke-width:2px
id5_1(Filter concatenation)
id5_2(1*1 conv)
id5_3(1*1 conv)
id5_4(3*3 conv)
id5_5(1*1 conv)
id5_6(5*5 conv)
id5_7(1*1 conv)
id5_8(3*3 pooling)
id5_9(Previous layer)
style id5_1 fill: #B9CF9320, stroke: #B9CF93, stroke-width:2px
style id5_9 fill: #B9CF9320, stroke: #B9CF93, stroke-width:2px
style id5_2 fill: #3991B620, stroke: #3991B6, stroke-width:2px
style id5_4 fill: #3991B620, stroke: #3991B6, stroke-width:2px
style id5_6 fill: #3991B620, stroke: #3991B6, stroke-width:2px
style id5_3 fill: #FFFF6940, stroke: #FFFF69, stroke-width:2px
style id5_5 fill: #FFFF6940, stroke: #FFFF69, stroke-width:2px
style id5_7 fill: #FFFF6940, stroke: #FFFF69, stroke-width:2px
style id5_8 fill: #CFA3A420, stroke: #CFA3A4, stroke-width:2px
subgraph block_1
id4_9 --&gt; id4_2
id4_2 --&gt; id4_1
id4_9 --&gt; id4_3
id4_3 --&gt; id4_4
id4_4 --&gt; id4_1
id4_9 --&gt; id4_5
id4_5 --&gt; id4_6
id4_6 --&gt; id4_1
id4_9 --&gt; id4_8
id4_8 --&gt; id4_7
id4_7 --&gt; id4_1
id5_9 --&gt; id5_2
id5_2 --&gt; id5_1
id5_9 --&gt; id5_3
id5_3 --&gt; id5_4
id5_4 --&gt; id5_1
id5_9 --&gt; id5_5
id5_5 --&gt; id5_6
id5_6 --&gt; id5_1
id5_9 --&gt; id5_8
id5_8 --&gt; id5_7
id5_7 --&gt; id5_1
id4_1 --&gt; id5_9
end
id1 --&gt; id2_9
id3_1 --&gt; id4_9
id6(global avgpool)
style id6 fill: #CFA3A420, stroke: #CFA3A4, stroke-width:2px
id7(Dense 10)
style id7 fill: #B9CF9320, stroke: #B9CF93, stroke-width:2px
id5_1 --&gt; id6
id6 --&gt; id7
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ConvBNRelu</span>(<span class="hljs-title class_ inherited__">Model</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, ch, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>, padding=<span class="hljs-string">&#x27;same&#x27;</span></span>):<br>        <span class="hljs-built_in">super</span>(ConvBNRelu, self).__init__()<br>        self.modle = tf.keras.models.Sequential([<br>            Conv2D(ch, kernel_size, strides=strides, padding=padding),<br>            BatchNormalization(),<br>            Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br>        ])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs, **kwargs</span>):<br>        outputs = self.modle(inputs, training=<span class="hljs-literal">False</span>)<br><br>        <span class="hljs-keyword">return</span> outputs<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InceptionBlock</span>(<span class="hljs-title class_ inherited__">Model</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, ch, strides=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>(InceptionBlock, self).__init__()<br>        self.ch = ch<br>        self.strides = strides<br>        self.c1 = ConvBNRelu(ch, kernel_size=<span class="hljs-number">1</span>, strides=strides)<br>        self.c2_1 = ConvBNRelu(ch, kernel_size=<span class="hljs-number">1</span>, strides=strides)<br>        self.c2_2 = ConvBNRelu(ch, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>)<br>        self.c3_1 = ConvBNRelu(ch, kernel_size=<span class="hljs-number">1</span>, strides=strides)<br>        self.c3_2 = ConvBNRelu(ch, kernel_size=<span class="hljs-number">5</span>, strides=<span class="hljs-number">1</span>)<br>        self.P4_1 = MaxPool2D(<span class="hljs-number">2</span>, strides=<span class="hljs-number">1</span>, padding=<span class="hljs-string">&#x27;same&#x27;</span>)<br>        self.c4_2 = ConvBNRelu(ch, kernel_size=<span class="hljs-number">1</span>, strides=strides)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs, training=<span class="hljs-literal">None</span>, mask=<span class="hljs-literal">None</span></span>):<br>        inputs1 = self.c1(inputs)<br>        inputs2_1 = self.c2_1(inputs)<br>        inputs2_2 = self.c2_2(inputs2_1)<br>        inputs3_1 = self.c3_1(inputs)<br>        inputs3_2 = self.c3_2(inputs3_1)<br>        inputs4_1 = self.P4_1(inputs)<br>        inputs4_2 = self.c4_2(inputs4_1)<br><br>         <span class="hljs-comment"># 沿深度方向堆叠</span><br>        outputs = tf.concat([inputs1, inputs2_2, inputs3_2, inputs4_2], axis=<span class="hljs-number">3</span>) <br><br>        <span class="hljs-keyword">return</span> outputs<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Inception10</span>(<span class="hljs-title class_ inherited__">Model</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_blocks, num_classes, init_ch=<span class="hljs-number">16</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Inception10, self).__init__(**kwargs)<br>        self.in_channels = init_ch<br>        self.out_channels = init_ch<br>        self.num_blocks = num_blocks<br>        self.init_ch = init_ch<br>        self.c1 = ConvBNRelu(init_ch)<br>        self.blocks = tf.keras.models.Sequential()<br>        <span class="hljs-keyword">for</span> block_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks):<br>            <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>                <span class="hljs-keyword">if</span> layer_id == <span class="hljs-number">0</span>:<br>                    block = InceptionBlock(self.out_channels, strides=<span class="hljs-number">2</span>)<br>                <span class="hljs-keyword">else</span>:<br>                    block = InceptionBlock(self.out_channels, strides=<span class="hljs-number">1</span>)<br>                self.blocks.add(block)<br><br>            self.out_channels *= <span class="hljs-number">2</span><br>        self.p1 = GlobalAveragePooling2D()<br>        self.f1 = Dense(num_classes, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs, **kwargs</span>):<br>        inputs = self.c1(inputs)<br>        inputs = self.blocks(inputs)<br>        inputs = self.p1(inputs)<br>        outputs = self.f1(inputs)<br><br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure><figure><img src="/images/posts/CNNmodels/InceptionNet.png" srcset="/images/loading.gif" lazyload alt="InceptionNet"><figcaption aria-hidden="true">InceptionNet</figcaption></figure><h2 id="resnet">5、ResNet</h2><p>将前面的输出特征结果越过对叠层直接传递到后面，并与堆叠卷积的非线性输出叠加，有效的缓解了神经网络模型堆叠导致的退化</p><pre><code class="mermaid">graph TB
    id0(( ))
    id1(3X3conv, filters=512&lt;br/&gt;strides=2)
    id2(3X3conv, filters=512)
    id3((&quot;H(x)&quot;))
    id4(3X3conv, filters=512)
    id5(3X3conv, filters=512)
    id6((&quot;H(x)&quot;))
    subgraph  
    id0 --&gt; id1
    id1 --&gt; id2
    id2 --&gt;|&quot;F(x)&quot;| id3
    end
    subgraph  
    id3 --&gt; id4
    id4 --&gt; id5
    id5 --&gt;|&quot;F(x)&quot;| id6
    id0 -.&quot;W(x)&quot;.-&gt; id3
    id3 --&gt;|x| id6
    end
</code></pre><p>其中，虚线表示维度不同时，此时H(x)=F(x)+W(x)，其中W(x)是1*1卷积操作，用于调整x的维度</p><p>实线表示维度相同时，此时H(x)=F(x)+x</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResnetBlock</span>(<span class="hljs-title class_ inherited__">Model</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, filters, strides=<span class="hljs-number">1</span>, residual_path=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>(ResnetBlock, self).__init__()<br>        self.filters = filters<br>        self.strides = strides<br>        self.residual_path = residual_path<br><br>        self.c1 = Conv2D(filters, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), strides=strides, padding=<span class="hljs-string">&#x27;same&#x27;</span>, use_bias=<span class="hljs-literal">False</span>)<br>        self.b1 = BatchNormalization()<br>        self.a1 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br><br>        self.c2 = Conv2D(filters, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), strides=<span class="hljs-number">1</span>, padding=<span class="hljs-string">&#x27;same&#x27;</span>, use_bias=<span class="hljs-literal">False</span>)<br>        self.b2 = BatchNormalization()<br><br>        <span class="hljs-comment"># residual_path为True时，对输入进行下采样，即用1x1的卷积核做卷积操作，保证x能和F(x)维度相同，顺利相加</span><br>        <span class="hljs-keyword">if</span> residual_path:<br>            self.down_c1 = Conv2D(filters, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), strides=strides, padding=<span class="hljs-string">&#x27;same&#x27;</span>, use_bias=<span class="hljs-literal">False</span>)<br>            self.down_b1 = BatchNormalization()<br><br>        self.a2 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs, **kwargs</span>):<br>        <span class="hljs-comment"># residual等于输入值本身，即residual=x</span><br>        residual = inputs<br><br>        <span class="hljs-comment"># 将输入通过卷积、BN层、激活层，计算F(x)</span><br>        x = self.c1(inputs)<br>        x = self.b1(x)<br>        x = self.a1(x)<br><br>        x = self.c2(x)<br>        y = self.b2(x)<br><br>        <span class="hljs-keyword">if</span> self.residual_path:<br>            residual = self.down_c1(inputs)<br>            residual = self.down_b1(residual)<br><br>        <span class="hljs-comment"># 最后输出的是两部分的和，即F(x)+x或F(x)+Wx,再过激活函数</span><br>        out = self.a2(y + residual)<br>        <span class="hljs-keyword">return</span> out<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResNet18</span>(<span class="hljs-title class_ inherited__">Model</span>):<br>    <span class="hljs-comment"># block_list表示每个block有几个卷积层</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, block_list, initial_filters=<span class="hljs-number">64</span></span>):<br>        <span class="hljs-built_in">super</span>(ResNet18, self).__init__()<br>        <span class="hljs-comment"># 共有几个block</span><br>        self.num_blocks = <span class="hljs-built_in">len</span>(block_list)<br>        self.block_list = block_list<br>        self.out_filters = initial_filters<br>        self.c1 = Conv2D(self.out_filters, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), strides=<span class="hljs-number">1</span>, padding=<span class="hljs-string">&#x27;same&#x27;</span>, use_bias=<span class="hljs-literal">False</span>)<br>        self.b1 = BatchNormalization()<br>        self.a1 = Activation(<span class="hljs-string">&#x27;relu&#x27;</span>)<br>        self.blocks = tf.keras.models.Sequential()<br>        <span class="hljs-comment"># 构建ResNet网络结构</span><br>        <span class="hljs-comment"># 第几个resnet block</span><br>        <span class="hljs-keyword">for</span> block_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(block_list)):<br>            <span class="hljs-comment"># 第几个卷积层</span><br>            <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(block_list[block_id]):<br>                <span class="hljs-comment"># 对除第一个block以外的每个block的输入进行下采样</span><br>                <span class="hljs-keyword">if</span> block_id != <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> layer_id == <span class="hljs-number">0</span>:<br>                    block = ResnetBlock(self.out_filters, strides=<span class="hljs-number">2</span>, residual_path=<span class="hljs-literal">True</span>)<br>                <span class="hljs-keyword">else</span>:<br>                    block = ResnetBlock(self.out_filters, residual_path=<span class="hljs-literal">False</span>)<br>                <span class="hljs-comment"># 将构建好的block加入resnet</span><br>                self.blocks.add(block)<br>            <span class="hljs-comment"># 下一个block的卷积核数是上一个block的2倍</span><br>            self.out_filters *= <span class="hljs-number">2</span><br>        self.p1 = GlobalAveragePooling2D()<br>        self.f1 = Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>, kernel_regularizer=tf.keras.regularizers.l2())<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, inputs, **kwargs</span>):<br>        x = self.c1(inputs)<br>        x = self.b1(x)<br>        x = self.a1(x)<br>        x = self.blocks(x)<br>        x = self.p1(x)<br>        y = self.f1(x)<br>        <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure><figure><img src="/images/posts/CNNmodels/ResNet.png" srcset="/images/loading.gif" lazyload alt="ResNet"><figcaption aria-hidden="true">ResNet</figcaption></figure></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E5%AD%A6%E4%B9%A0/" class="category-chain-item">学习</a> <span>></span> <a href="/categories/%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E5%AD%A6%E4%B9%A0/" class="print-no-link">#学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a></div></div><div class="license-box my-3"><div class="license-title"><div>几种经典的CNN模型</div><div>https://www.aye10032.com/2020/12/29/2020-12-29-CNNmodles/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Aye10032</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2020年12月29日</div></div><div class="license-meta-item license-meta-date"><div>更新于</div><div>2023年3月15日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2021/02/19/2021-02-19-stellaris/" title="群星各政体（种族）岗位产出一览"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">群星各政体（种族）岗位产出一览</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2020/12/21/2020-12-21-BLEearphone/" title="WIN10关闭蓝牙耳机绝对音量功能"><span class="hidden-mobile">WIN10关闭蓝牙耳机绝对音量功能</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="gitalk-container"></div><script type="text/javascript">Fluid.utils.loadComments("#gitalk-container",(function(){Fluid.utils.createCssLink("/css/gitalk.css"),Fluid.utils.createScript("https://lib.baomitu.com/gitalk/1.8.0/gitalk.min.js",(function(){var e=Object.assign({clientID:"eeda953af44e09f78441",clientSecret:"cb84da4697fdc74e72451e5b9618297b1667988b",repo:"Aye10032.github.io",owner:"Aye10032",admin:["Aye10032"],language:"zh-CN",labels:["Gitalk"],perPage:10,pagerDirection:"last",distractionFreeMode:!1,createIssueManually:!0,proxy:"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},{id:"864e1f28cb500b4916fc92561cdde66b"});new Gitalk(e).render("gitalk-container")}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><script>Fluid.utils.createScript("https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js",(function(){mermaid.initialize({theme:"default"}),Fluid.utils.listenDOMLoaded((function(){Fluid.events.registerRefreshCallback((function(){"mermaid"in window&&mermaid.init()}))}))}))</script><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div><div class="beian"><span><a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">苏ICP备19040408号-2 </a></span><span><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=32108102010462" rel="nofollow noopener" class="beian-police" target="_blank"><span style="visibility:hidden;width:0">|</span> <img src="/images/%E5%A4%87%E6%A1%88%E5%9B%BE%E6%A0%87.png" srcset="/images/loading.gif" lazyload alt="police-icon"> <span>苏公网安备 32108102010462号</span></a></span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>